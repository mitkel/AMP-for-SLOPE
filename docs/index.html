<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Bartłomiej Polaczyk" />

<meta name="date" content="2020-07-18" />

<title>AMP for SLOPE</title>

<script src="report_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="report_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="report_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="report_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="report_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="report_files/navigation-1.1/tabsets.js"></script>
<link href="report_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="report_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>


<style type="text/css">
  p.abstract{
    text-align: center;
    font-weight: bold;
  }
  div.abstract{
    margin: auto;
    width: 90%;
  }
</style>

<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="AMP-SLOPE.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">AMP for SLOPE</h1>
<h4 class="author"><em>Bartłomiej Polaczyk</em></h4>
<h4 class="date"><em>18 July 2020</em></h4>
<div class="abstract">
<p class="abstract">Abstract</p>
<p>The aim of this project is to investigate the the approximate message passing algorithm for SLOPE regularization problem based on <span class="citation">(Bu et al. 2019)</span> and compare it with classical convex optimization methods. Some numerical experiments regarding the cases that do not fit into the theoretical framework of <span class="citation">(Bu et al. 2019)</span> are also performed and analyzed.</p>
</div>

</div>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<div id="theoretical-bacground" class="section level1">
<h1>Theoretical bacground</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
We are interested in solving the standard linear inverse problem
<span class="math display">\[\begin{equation}\label{eq:LM-problem} 
  y = Ax + w,
\end{equation}\]</span>
<p>where <span class="math inline">\(y\in {\mathbb{R}}^n\)</span> and <span class="math inline">\(A\in{\mathbb{R}}^{n\times p}\)</span> are known parameters of the model, <span class="math inline">\(w\in{\mathbb{R}}^n\)</span> is a random noise vector and <span class="math inline">\(x\in{\mathbb{R}}^p\)</span> is an unknown vector of paramteres we wish to estimate. We assume <span class="math inline">\(p\gg n\)</span>, i.e. the number of features is much greater than the size of the sample data and whence there might be many potential solutions to the problem<span class="math inline">\(~\eqref{eq:LM-problem}\)</span>.</p>
To resolve this issue and prevent overfitting, we introduce a penalty function <span class="math inline">\(\phi\)</span> which faforizes sparse solutions of<span class="math inline">\(~\eqref{eq:LM-problem}\)</span>, i.e. now we are looking among the minimizers of the following form
<span class="math display">\[\begin{equation}\label{eq:SLOPE}
  \widehat{x} = \operatorname*{argmin}_x \{\, \frac{1}{2}\Vert Ax - y \Vert_2^2 + \phi(x) \,\}.
\end{equation}\]</span>
<p>The usual choices of <span class="math inline">\(\phi\)</span> are scaled <span class="math inline">\(l^2\)</span> penalty (Tikhonov regularization) and <span class="math inline">\(l^1\)</span> penalty (LASSO). This note concerns sorted <span class="math inline">\(l^1\)</span> penalized estimation (abbrev. SLOPE), introduced for the first time in <span class="citation">(Bogdan et al. 2015)</span>, which assumes <span class="math inline">\(\phi\)</span> to be the sorted <span class="math inline">\(l^1\)</span> penalty, i.e. <span class="math display">\[
  \phi(x)= \phi_{\lambda}(x) = \sum_{i=1}^n \lambda_ix_i^{\downarrow},
\]</span> where <span class="math inline">\(x_1^\downarrow \ge x_2^\downarrow \ge \ldots \ge x_n^\downarrow\)</span> is the ordered permutation of the vector <span class="math inline">\({\left\vert x \right\vert}=({\left\vert x_1 \right\vert},{\left\vert x_2 \right\vert},\ldots,{\left\vert x_n \right\vert})\)</span> and <span class="math inline">\(\lambda_1 \ge \lambda_2 \ge \ldots \lambda_p \ge 0\)</span> are hyperparameters of the model. To lighten notation, we denote <span class="math inline">\(\Delta_p = \{\, x\in{\mathbb{R}}^p\colon x_1\ge x_2 \ge \ldots \ge x_p \ge 0 \,\}\)</span>, so that the above requirements read: <span class="math inline">\(x^\downarrow,\lambda\in\Delta_p\)</span>, where <span class="math inline">\(x^\downarrow = P{\left\vert x \right\vert}\)</span> for some permutation <span class="math inline">\(P\)</span> of the set <span class="math inline">\(\{1,2,\ldots,p\}\)</span>. Such a choice of regulizer is a generalization of the <span class="math inline">\(l^1\)</span> regularization, as can be seen by taking <span class="math inline">\(\lambda_i\equiv\operatorname{const}\)</span>.</p>
<p>The fact that <span class="math inline">\(\phi_\lambda\)</span> is non-separable makes the analysis of its teoretical properties much more onerous than in case of classical (separable) models, cf. <span class="citation">(Bogdan et al. 2015, <span class="citation">Bu et al. (2019)</span>)</span>. Nonetheless, it turns out that SLOPE has two advantages over other regularization methods such as LASSO and knocoffs, namely:</p>
<ol style="list-style-type: decimal">
<li>it achieves certain minimax estimation properties under particular random designs <em>without</em> requiring any knowledge of the sparsity degree of <span class="math inline">\(\widehat{x}\)</span>, cf. <span class="citation">(Bellec, Lecué, and Tsybakov 2018)</span>;</li>
<li>it controls the false discovery rate in the case of independent predictors, cf. <span class="citation">(Bogdan et al. 2015)</span>.</li>
</ol>
<p>We are interested in the algorithmic solutions to the problem <span class="math inline">\(\eqref{eq:SLOPE}\)</span>. Since the objective function in <span class="math inline">\(\eqref{eq:SLOPE}\)</span> is convex but not smooth, one can not apply directly the classical gradient descent and has to turn to other methods.</p>
<p>A natural alternative solution is the plethora of proximal algorithms, e.g. ISTA (and its improvement – FISTA, cf. <span class="citation">(A. Beck and Teboulle 2009)</span>) or more classical alternating direction methods of multipliers (ADMM). The methods have been throughly studied in the literature, cf. <span class="citation">(Beck A 2017)</span> for a detailed treatment of the subject.</p>
<p>In this note we will focus on another approach, via the approximate message passing, considered for the first time in context of the LASSO problem in <span class="citation">(Donoho, Maleki, and Montanari 2009)</span> and subsequentially developed in e.g. <span class="citation">(Bayati and Montanari 2011)</span>, and for the SLOPE regularization in <span class="citation">(Bu et al. 2019)</span> – see e.g. <span class="citation">(Zdeborová and Krzakala 2016)</span> for an accessible derivation of the method.</p>
<p>In the subsequent sections we will describre briefly some of these approaches.</p>
</div>
<div id="proximal-methods" class="section level2">
<h2>Proximal methods</h2>
<p>Denoting <span class="math inline">\(g(x)=\frac{1}{2}\Vert Ax - y \Vert_2^2\)</span>, the iterative shrinkage thresholding algorithm (ISTA) iteration for SLOPE with given <span class="math inline">\(\lambda\)</span> can be written as:</p>
<hr />
<p><strong>ISTA-SLOPE:</strong><br> <strong>Input:</strong> <span class="math inline">\(y\in{\mathbb{R}}^p\)</span>, <span class="math inline">\(\lambda\in\Delta_p\)</span>, <span class="math inline">\(A\in{\mathbb{R}}^{n\times p}\)</span><br> Initialize <span class="math inline">\(g(x)=\frac{1}{2}{\left\Vert Ax-y \right\Vert}^2\)</span>, <span class="math inline">\(x\in{\mathbb{R}}^p\)</span>, <span class="math inline">\(t&gt;0\)</span> <br> <strong>while</strong> (<em>stopping condition</em>) <strong>{</strong> <br>   <span class="math inline">\(x \leftarrow {\operatorname{prox}}_{t\phi_\lambda}\big(x - t\nabla g(x)\big)\)</span>;<br> <strong>} return</strong> <span class="math inline">\(x\)</span></p>
<hr />
<p>where <span class="math inline">\(t\)</span> can be thought of as the learning rate and <span class="math inline">\(\operatorname{prox}\)</span> denotes the proximal operator given by <span class="math display">\[
  {\operatorname{prox}}_{h}(y) := \operatorname*{argmin}_x \{\, h(x) + \frac{1}{2}\Vert x-y \Vert_2^2 \,\}.
\]</span></p>
<p><span class="citation">A. Beck and Teboulle (2009)</span> have introduced a faster version of ISTA, a.k.a. FISTA, which is based on the idea of Nesterov momentum. The general form of the algorithm is the following:</p>
<hr />
<p><strong>FISTA-SLOPE:</strong><br> <strong>Input:</strong> <span class="math inline">\(y\in{\mathbb{R}}^p\)</span>, <span class="math inline">\(\lambda\in\Delta_p\)</span>, <span class="math inline">\(A\in{\mathbb{R}}^{n\times p}\)</span><br> Initialize <span class="math inline">\(g(x)=\frac{1}{2}{\left\Vert Ax-y \right\Vert}^2\)</span>, <span class="math inline">\(x=x_{old}\in{\mathbb{R}}^p\)</span>, <span class="math inline">\(r = 1\)</span>, <span class="math inline">\(t&gt;0\)</span><br> <strong>while</strong> <em>(stopping condition)</em> <strong>{</strong><br>   <span class="math inline">\(u \leftarrow x_{old} + r(x-x_{old})\)</span>;<br>   <span class="math inline">\(x_{old}\leftarrow x\)</span>;<br>   <span class="math inline">\(x \leftarrow {\operatorname{prox}}_{t\phi_\lambda}\big(u - t\nabla g(u)\big)\)</span>;<br>   <em>update</em>(<span class="math inline">\(r\)</span>);<br> <strong>} return</strong> <span class="math inline">\(x\)</span></p>
<hr />
<p>Here <span class="math inline">\(t\)</span> can be thought of as a acceleration term, which (if updated correctly through the update rule) can increase substantialy the speed of convergence of the algorithm. Note that keeping <span class="math inline">\(r\equiv 1\)</span> restores ISTA.</p>
<p>One of the difficulties in dealing with SLOPE is that the regulizer <span class="math inline">\(\phi_\lambda\)</span> is non-separable and thus its proximal operator cannot be applied element-wise. <span class="citation">(Bogdan et al. 2015)</span> have proposed an efficient algorithm that for any <span class="math inline">\(u\in{\mathbb{R}}^p\)</span> computes <span class="math display">\[
 \widehat{x}=
 {\operatorname{prox}}_{\phi_{\lambda}}(u) = 
 \operatorname*{argmin}_x \big\{ \frac{1}{2}{\left\Vert u-x \right\Vert}^2 + \sum_i \lambda_ix_i^\downarrow \big\}
 =
 \operatorname*{argmin}_x \big\{\sum_i\big[ \frac{1}{2}(x^\downarrow_i)^2 + x_i^\downarrow\lambda_i - x_iu_i\big]\big\}
\]</span> It is based on the following simple observations that follow immediatly from the above formulation:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\({\operatorname{sign}}(\widehat{x}_i)={\operatorname{sign}}(u_i)\)</span> for each <span class="math inline">\(i\)</span> such that <span class="math inline">\(\widehat{x}_i\neq 0\)</span>;</li>
<li><span class="math inline">\(P\widehat{x}={\operatorname{prox}}_{\phi_\lambda}(Pu)\)</span> for any permutation <span class="math inline">\(P\)</span>;</li>
<li>If <span class="math inline">\(u\in \Delta_p\)</span>, then <span class="math inline">\(\widehat{x}\in\Delta_p\)</span> (i.e., <span class="math inline">\(u=u^\downarrow \Rightarrow \widehat{x}=\widehat{x}^\downarrow\)</span>);</li>
</ol>
<p>Therefore we can and do assume in the analysis below that <span class="math inline">\(u\in\Delta_p\)</span>. The optimization procedure now reads: <span class="math display">\[
  \widehat{x} = 
  \operatorname*{argmin}_{x^\downarrow\in\Delta_p} \big\{\sum_i\big[ \frac{1}{2}(x^\downarrow_i)^2 - x_i^\downarrow(u-\lambda)_i \big]\big\},
\]</span> whence</p>
<ol start="4" style="list-style-type: decimal">
<li><span class="math inline">\(\widehat{x}\)</span> depends only on the vector <span class="math inline">\((\lambda - u)\)</span>;</li>
<li>If <span class="math inline">\((u - \lambda)_+\in\Delta_p\)</span> then <span class="math inline">\(\widehat{x} = (u - \lambda)_+\)</span>, where <span class="math inline">\(v_+ = (\max(v_i,0))_i\)</span>;</li>
<li>If <span class="math inline">\((u-\lambda)_i \le (u-\lambda)_{i+1}\)</span>, then <span class="math inline">\(\widehat{x}_i \le \widehat{x}_{i+1}\)</span>, whence <span class="math inline">\(\widehat{x}_i = \widehat{x}_{i+1}\)</span>;</li>
<li>Consequently, if <span class="math inline">\((u-\lambda)\)</span> is nondecreasing along the indices <span class="math inline">\((i,i+1,\ldots,j)\)</span>, then <span class="math inline">\(\widehat{x}_i = \widehat{x}_{i+1}= \ldots = \widehat{x}_{i+1}\)</span> and <span class="math display">\[
  \sum_{i\le r \le j}  \widehat{x}_r^\downarrow(u-\lambda)_r 
  =
  \sum_{i\le r \le j} \widehat{x}_r^\downarrow(\bar{u}-\bar{\lambda} ) \,
\]</span> where <span class="math inline">\(\bar{u},\bar{\lambda}\)</span> are the means of <span class="math inline">\(u\)</span> and <span class="math inline">\(\lambda\)</span> along the indices <span class="math inline">\((i,i+1,\ldots,j)\)</span>, so replacing <span class="math inline">\(u_r,\lambda_r\)</span> with <span class="math inline">\(\bar{u},\bar{\lambda}\)</span> does not change <span class="math inline">\(\widehat{x}\)</span>.</li>
</ol>
<p>The above observations justify the procedure below proposed by <span class="citation">(Bogdan et al. 2015)</span>.</p>
<hr />
<p><strong>FastProxSL1:</strong><br> <strong>Input:</strong> <span class="math inline">\(u\in{\mathbb{R}}^p\)</span>, <span class="math inline">\(\lambda\in\Delta_p\)</span><br> # Define the operator <span class="math inline">\(H_u(v) = P({\operatorname{sign}}(u_i)v_i)_i\)</span> for some permutation <span class="math inline">\(P\)</span>, so that <span class="math inline">\(H_u(u) = u^\downarrow\in\Delta_p\)</span><br> <span class="math inline">\(u&#39; \leftarrow H_u(u)\)</span>;<br> <strong>while</strong> <span class="math inline">\((u&#39;-\lambda)_+\notin\Delta_p\)</span> <strong>{</strong><br>   identify nondecreasing and nonconstant segments <span class="math inline">\(i:j\)</span> of <span class="math inline">\((u&#39;-\lambda)\)</span><br>   replace <span class="math inline">\(u&#39;_r,\lambda_r\)</span> for <span class="math inline">\(r\in\{i,i+1,\ldots,j\}\)</span> by their averages <span class="math inline">\(\bar{u&#39;},\bar{\lambda}\)</span> <br> <strong>} return</strong> <span class="math inline">\(H_u^{-1}(u&#39;-\lambda)_+\)</span>;</p>
<hr />
</div>
<div id="approximate-message-passing" class="section level2">
<h2>Approximate message passing</h2>
<p>The AMP approach is Bayes in nature. Namely, we assume that the true values of <span class="math inline">\(x\)</span> are i.i.d. from some apriori distribution <span class="math inline">\(X=(X_1,\ldots,X_p)\)</span> satisfying some intregrability choditions and that the noise vector <span class="math inline">\(w\)</span> is elementwise i.i.d. with zero mean and variance <span class="math inline">\(\sigma_w^2&lt;\infty\)</span>. The apriori distribution <span class="math inline">\(X\)</span> and the parameter <span class="math inline">\(\sigma_w^2\)</span> will play a crucial role in the construction of the algorithm as will be seen in a while. The base of the AMP algorithm for SLOPE is as follows</p>
<hr />
<p><strong>AMP-SLOPE:</strong><br> <strong>Input:</strong> <span class="math inline">\(y\in{\mathbb{R}}^p\)</span>, <span class="math inline">\(\lambda\in\Delta_p\)</span>, <span class="math inline">\(A\in{\mathbb{R}}^{n\times p}\)</span><br> Initialize: <span class="math inline">\(g(x)=\frac{1}{2}{\left\Vert Ax-y \right\Vert}^2\)</span>, <span class="math inline">\(x=x_{old}\in{\mathbb{R}}^p\)</span>, <span class="math inline">\(v=\nabla g(0)\)</span>, <span class="math inline">\(t=t(X)\in{\mathbb{R}}_+\)</span><br> <strong>while</strong> <em>(stopping condition)</em> <strong>{</strong><br>   <span class="math inline">\(x_{old}\leftarrow x\)</span>;<br>   <span class="math inline">\(x \leftarrow {\operatorname{prox}}_{\phi_{t\lambda}}\big(x_{old} - v\big)\)</span>;<br>   <span class="math inline">\(v \leftarrow \nabla g(x) - \frac{v}{n} [\nabla{\operatorname{prox}}_{\phi_{t\lambda}}(x_{old} - v)]\)</span>;<br>   <em>update</em>(<span class="math inline">\(t\)</span>);<br> <strong>} return</strong> <span class="math inline">\(x\)</span></p>
<hr />
<p>Based on the observations 1.-7. above, it is easy to verify that for any vector <span class="math inline">\(u\in{\mathbb{R}}^p\)</span> <span class="math display">\[
  \nabla{\operatorname{prox}}_{\phi_\lambda}(u) = 
  \Vert {\operatorname{prox}}_{\phi_\lambda}(u) \Vert_0^\ast,
  \quad\text{where}\quad
  {\left\Vert u \right\Vert}_0^\ast:= 
  \#\{ \text{unique non-zero magintudes in } {\left\vert u \right\vert} \}.
\]</span> E.g., <span class="math inline">\({\left\Vert (0,3,-3,3,1) \right\Vert}_0^\ast = 2\)</span>. Therefore, <span class="math inline">\(v\)</span> update in the AMP-SLOPE algorithm can be read as <span class="math display">\[
 v \leftarrow \nabla g(x) - \frac{v}{n} {\left\Vert x \right\Vert}_0^\ast.
\]</span></p>
Moreover, the scaling factor <span class="math inline">\(t\)</span> and its update rule in the AMP scheme are dictated by the so-called state evolution equation. Namely, let <span class="math display">\[
  F(\tau, \alpha) = \sigma_w^2 + \frac{1}{n} {\mathbb{E}}{\left\Vert {\operatorname{prox}}_{\phi_{\tau\alpha}}(X+\tau Z) - X \right\Vert}^2,
  \quad
  \tau\in{\mathbb{R}}_+,\,\alpha\in\Delta^p,
\]</span> where <span class="math inline">\(Z\)</span> is the <span class="math inline">\(\mathcal{N}(0,I_p)\)</span> random vector independent on the whole model. Then, setting
<span class="math display">\[\begin{equation}\label{eq:SE}
\tau_0=\sigma_w^2 + \frac{p}{n}\mathbb{E} X_1^2,
\quad
\tau_{k+1} = F(\tau_k,\alpha),
\quad
\tau_\ast = \lim_k \tau_k
\end{equation}\]</span>
we choose <span class="math inline">\(\hat{\alpha}\)</span> to be the solution of
<span class="math display">\[\begin{equation}\label{eq:lambda-alpha}
  \lambda = \alpha\tau_\ast\big( 1- \frac{1}{n}\mathbb{E}\Vert{\operatorname{prox}_{\phi_{\tau_\ast\alpha}}(X+\tau_\ast Z)}\Vert_0^\ast \big)
\end{equation}\]</span>
<p>and finally <span class="math display">\[
 t = \frac{\tau}{\tau^\ast\bigl(1-\frac{1}{n}{\mathbb{E}}\Vert{{\operatorname{prox}}_{\phi_{\tau_\ast\hat{\alpha}}}(X+\tau^\ast Z)}\Vert_0^\ast  \bigr)}.
\]</span></p>
<p><span class="citation">Bu et al. (2019)</span> have shown under some technical assumptions that there is a unique fixed point of the state evolution~ (i.e., that <span class="math inline">\(\tau^\ast\)</span> is well defined), and that the solution <span class="math inline">\(\hat{\alpha}\)</span> to~ is unique and well-defined. They have also proposed a numerical scheme for calculating <span class="math inline">\(\hat{\alpha}\)</span> based on the bisection method which we will exploit.</p>
</div>
</div>
<div id="numerical-experiments" class="section level1">
<h1>Numerical experiments</h1>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-MR2810285">
<p>Bayati, M., and A. Montanari. 2011. “The Dynamics of Message Passing on Dense Graphs, with Applications to Compressed Sensing.” <em>IEEE Trans. Inform. Theory</em> 57 (2): 764–85. doi:<a href="https://doi.org/10.1109/TIT.2010.2094817">10.1109/TIT.2010.2094817</a>.</p>
</div>
<div id="ref-MR2486527">
<p>Beck, A., and M. Teboulle. 2009. “A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems.” <em>SIAM J. Imaging Sci.</em> 2 (1): 183–202. doi:<a href="https://doi.org/10.1137/080716542">10.1137/080716542</a>.</p>
</div>
<div id="ref-MR3719240">
<p>Beck, A. 2017. <em>First-Order Methods in Optimization</em>. Vol. 25. MOS-Siam Series on Optimization. Society for Industrial; Applied Mathematics (SIAM), Philadelphia, PA; Mathematical Optimization Society, Philadelphia, PA. doi:<a href="https://doi.org/10.1137/1.9781611974997.ch1">10.1137/1.9781611974997.ch1</a>.</p>
</div>
<div id="ref-MR3852663">
<p>Bellec, P. C., G. Lecué, and A. B. Tsybakov. 2018. “Slope Meets Lasso: Improved Oracle Bounds and Optimality.” <em>Ann. Statist.</em> 46 (6B): 3603–42. doi:<a href="https://doi.org/10.1214/17-AOS1670">10.1214/17-AOS1670</a>.</p>
</div>
<div id="ref-MR3418717">
<p>Bogdan, M., E. van den Berg, C. Sabatti, W. Su, and E. J. Candès. 2015. “SLOPE—adaptive Variable Selection via Convex Optimization.” <em>Ann. Appl. Stat.</em> 9 (3): 1103–40. doi:<a href="https://doi.org/10.1214/15-AOAS842">10.1214/15-AOAS842</a>.</p>
</div>
<div id="ref-bu2019algorithmic">
<p>Bu, Z., J. Klusowski, C. Rush, and W. Su. 2019. “Algorithmic Analysis and Statistical Estimation of Slope via Approximate Message Passing.” In <em>Advances in Neural Information Processing Systems</em>, 9366–76. <a href="http://par.nsf.gov/biblio/10163278" class="uri">http://par.nsf.gov/biblio/10163278</a>.</p>
</div>
<div id="ref-donoho2009message">
<p>Donoho, D., A. Maleki, and A. Montanari. 2009. “Message-Passing Algorithms for Compressed Sensing.” <em>Proceedings of the National Academy of Sciences</em> 106 (45). National Acad Sciences: 18914–9. doi:<a href="https://doi.org/https://doi.org/10.1073/pnas.0909892106">https://doi.org/10.1073/pnas.0909892106</a>.</p>
</div>
<div id="ref-zdeborova2016statistical">
<p>Zdeborová, L., and F. Krzakala. 2016. “Statistical Physics of Inference: Thresholds and Algorithms.” <em>Advances in Physics</em> 65 (5). Taylor &amp; Francis: 453–552. doi:<a href="https://doi.org/https://doi.org/10.1080/00018732.2016.1211393">https://doi.org/10.1080/00018732.2016.1211393</a>.</p>
</div>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "report_files/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
