---
title: "AMP for SLOPE"
author: "Bart≈Çomiej Polaczyk"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  # pdf_document:
  #   latex_engine: xelatex
  html_document:
   css: AMP-SLOPE.css
   mathjax: local
   self_contained: false
abstract: |
  The aim of this project is to investigate the the approximate message passing algorithm for SLOPE regularization problem based on [@bu2019algorithmic] and compare it with classical convex optimization methods.
  Some numerical experiments regarding the cases that do not fit into the theoretical framework of [@bu2019algorithmic] are also performed and analyzed.
bibliography: AMP-SLOPE.bib
header-includes:
  - \usepackage{mathtools}
  # - \usepackage[ruled,vlined]{algorithm2e}
---
%LaTeX commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\prox}{\operatorname{prox}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\abs}[1]{\left\vert #1 \right\vert}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}

%numbering equations in html format
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Libraries
packages <- c("foreign", "lubridate", "ggplot2","reshape2")
lapply(packages, library, character.only = TRUE)

# working directory
#setwd("~/Dropbox/Uczelnia//")
```


# Theoretical bacground
## Introduction
We are interested in solving the standard linear inverse problem
\begin{equation}\label{eq:LM-problem} 
  y = Ax + w,
\end{equation} 
where $y\in \R^n$ and $A\in\R^{n\times p}$ are known parameters of the model, $w\in\R^n$ is a random noise vector and $x\in\R^p$ is an unknown vector of paramteres we wish to estimate.
We assume $p\gg n$, i.e. the number of features is much greater than the size of the sample data and whence there might be many potential solutions to the problem$~\eqref{eq:LM-problem}$.

To resolve this issue and prevent overfitting, we introduce a penalty function $\phi$ which faforizes sparse solutions of$~\eqref{eq:LM-problem}$, i.e. now we are looking among the minimizers of the following form
\begin{equation}\label{eq:SLOPE}
  \widehat{x} = \operatorname*{argmin}_x \{\, \frac{1}{2}\Vert Ax - y \Vert_2^2 + \phi(x) \,\}.
\end{equation}
The usual choices of $\phi$ are scaled $l^2$ penalty (Tikhonov regularization) and $l^1$ penalty (LASSO).
This note concerns sorted $l^1$ penalized estimation (abbrev. SLOPE), introduced for the first time in [@MR3418717], which assumes $\phi$ to be the sorted $l^1$ penalty, i.e.
$$
  \phi(x)= \phi_{\theta}(x) = \sum_{i=1}^n \theta_ix_i^{\downarrow},
$$
where $x_1^\downarrow \ge x_2^\downarrow \ge \ldots \ge x_n^\downarrow$ is the ordered permutation of the vector $\abs{x}=(\abs{x_1},\abs{x_2},\ldots,\abs{x_n})$ and $\theta_1 \ge \theta_2 \ge \ldots \theta_p \ge 0$ are hyperparameters of the model.
To lighten notation, we denote $\Delta_p = \{\, x\in\R^p\colon x_1\ge x_2 \ge \ldots \ge x_p \ge 0 \,\}$, so that the above requirements read: $x^\downarrow,\theta\in\Delta_p$, where $x^\downarrow = P\abs{x}$ for some permutation $P$ of the set $\{1,2,\ldots,p\}$.
Such a choice of regulizer is a generalization of the $l^1$ regularization, as can be seen by taking $\theta_i\equiv\operatorname{const}$.

The fact that $\phi_\theta$ is non-separable makes the analysis of its teoretical properties much more onerous than in case of classical (separable) models, cf. [@MR3418717, @bu2019algorithmic].
Nonetheless, it turns out that SLOPE has two advantages over other regularization methods such as LASSO and knocoffs, namely:

1. it achieves certain minimax estimation properties under particular random designs *without* requiring any knowledge of the sparsity degree of $\widehat{x}$, cf. [@MR3852663];
2. it controls the false discovery rate in the case of independent predictors, cf. [@MR3418717].

We are interested in the algorithmic solutions to the problem $\eqref{eq:SLOPE}$.
Since the objective function in $\eqref{eq:SLOPE}$ is convex but not smooth, one can not apply directly the classical gradient descent and has to turn to other methods.

A natural alternative solution is the plethora of proximal algorithms, e.g. ISTA (and its improvement -- FISTA, cf. [@MR2486527]) or more classical alternating direction methods of multipliers (ADMM).
The methods have been throughly studied in the literature, cf. [@MR3719240] for a detailed treatment of the subject.

In this note we will focus on another approach, via the approximate message passing, considered for the first time in context of the LASSO problem in [@donoho2009message] and subsequentially developed in e.g. [@MR2810285], and for the SLOPE regularization in [@bu2019algorithmic] -- see e.g. [@zdeborova2016statistical] for an accessible derivation of the method.

In the subsequent sections we will describre briefly some of these approaches.

## Proximal methods

Denoting $g(x)=\frac{1}{2}\Vert Ax - y \Vert_2^2$, the iterative shrinkage thresholding algorithm (ISTA) iteration for SLOPE with given $\theta$ can be written as:

******
**ISTA-SLOPE:**<br>
**Input:** $y\in\R^p$, $\theta\in\Delta_p$, $g$<br>
Initialize $x\in\R^p$, $t>0$ <br>
**while** (*stopping condition*) **{** <br>
&emsp;  $x = \prox_{t\phi_\theta}\big(x - t\nabla g(x)\big)$;<br>
**} return** $x$

******

where $t$ can be thought of as the learning rate, *update* is an appropriate backtracking procedure and $\operatorname{prox}$ denotes the proximal operator given by
$$
  \prox_{h}(y) := \operatorname*{argmin}_x \{\, h(x) + \frac{1}{2}\Vert x-y \Vert_2^2 \,\}.
$$

@MR2486527 have introduced a faster version of ISTA, a.k.a. FISTA, which is based on the idea of Nesterov momentum.
The general form of the algorithm is the following:

******
**FISTA-SLOPE:**<br>
**Input:** $y\in\R^p$, $\theta\in\Delta_p$, $g$<br>
Initialize $x=x_{old}\in\R^p$, $r = 1$, $t>0$<br>
**while** *(stopping condition)* **{**<br>
&emsp;  $u = x_{old} + r(x-x_{old})$;<br>
&emsp;  $x_{old}=x$;<br>
&emsp;  $x = \prox_{t\phi_\theta}\big(u - t\nabla g(u)\big)$;<br>
&emsp;  *update*($r$);<br>
**} return** $x$

******

Here $t$ can be thought of as a acceleration term, which (if updated correctly) can increase substantialy the speed of convergence of the algorithm.
Note that keeping $t\equiv 1$ restores ISTA.

One of the difficulties in dealing with SLOPE is that the regulizer $\phi_\theta$ is non-separable and thus its proximal operator cannot be applied element-wise.
[@MR3418717] have proposed an efficient algorithm that for any $u\in\R^p$ computes 
$$
 \widehat{x}=
 \prox_{\phi_{\theta}}(u) = 
 \operatorname*{argmin}_x \big\{ \frac{1}{2}\norm{u-x}^2 + \sum_i \theta_ix_i^\downarrow \big\}
 =
 \operatorname*{argmin}_x \big\{\sum_i\big[ \frac{1}{2}(x^\downarrow_i)^2 + x_i^\downarrow\theta_i - x_iu_i\big]\big\}
$$
It is based on the following simple observations that follow immediatly from the above formulation:

1. $\sign(\widehat{x}_i)=\sign(u_i)$ for each $i$ such that $\widehat{x}_i\neq 0$;
2. $P\widehat{x}=\prox_{\phi_\theta}(Pu)$ for any permutation $P$;
3. If $u\in \Delta_p$, then $\widehat{x}\in\Delta_p$ (i.e., $u=u^\downarrow \Rightarrow \widehat{x}=\widehat{x}^\downarrow$);

Therefore we can and do assume in the analysis below that $u\in\Delta_p$.
The optimization procedure now reads:
$$
  \widehat{x} = 
  \operatorname*{argmin}_{x^\downarrow\in\Delta_p} \big\{\sum_i\big[ \frac{1}{2}(x^\downarrow_i)^2 - x_i^\downarrow(u-\theta)_i \big]\big\},
$$
whence

4. $\widehat{x}$ depends only on the vector $(\theta - u)$;
5. If $(u - \theta)_+\in\Delta_p$ then $\widehat{x} = (u - \theta)_+$, where $v_+ = (\max(v_i,0))_i$;
6. If $(u-\theta)_i \le (u-\theta)_{i+1}$, then $\widehat{x}_i \le \widehat{x}_{i+1}$, whence $\widehat{x}_i = \widehat{x}_{i+1}$;
7. Consequently, if $(u-\theta)$ is nondecreasing along the indices $(i,i+1,\ldots,j)$, then $\widehat{x}_i = \widehat{x}_{i+1}= \ldots = \widehat{x}_{i+1}$ and 
$$
  \sum_{i\le r \le j}  \widehat{x}_r^\downarrow(u-\theta)_r 
  =
  \sum_{i\le r \le j} \widehat{x}_r^\downarrow(\bar{u}-\bar{\theta} ) \,
$$
where $\bar{u},\bar{\theta}$ are the means of $u$ and $\theta$ along the indices $(i,i+1,\ldots,j)$, so replacing $u_r,\theta_r$ with $\bar{u},\bar{\theta}$ does not change $\widehat{x}$.


The above observations justify the procedure below proposed by [@MR3418717].

******
**FastProxSL1:**<br>
**Input:** $u\in\R^p$, $\theta\in\Delta_p$<br>
\# Define the operator $H_u(v) = P(\sign(u_i)v_i)_i$ for some permutation $P$, so that $H_u(u) = u^\downarrow\in\Delta_p$<br>
$u' = H_u(u)$;<br>
**while** $(u'-\theta)_+\notin\Delta_p$ **{**<br>
&emsp; identify nondecreasing and nonconstant segments $i:j$ of $(u'-\theta)$<br>
&emsp; replace $u'_r,\theta_r$ for $r\in\{i,i+1,\ldots,j\}$ by their averages $\bar{u'},\bar{\theta}$ <br>
**} return** $H_u^{-1}(u'-\theta)_+$;

******

## Approximate message passing

The AMP algorithm for SLOPE is as follows

******
**AMP-SLOPE:**<br>
**Input:** $y\in\R^p$, $\theta\in\Delta_p$, $g$<br>
Initialize $x=x_{old}\in\R^p$, $v=\nabla g(0)$<br>
**while** *(stopping condition)* **{**<br>
&emsp;  $x_{old}=x$;<br>
&emsp;  $x = \prox_{\phi_\theta}\big(x_{old} - v\big)$;<br>
&emsp;  $v = \nabla g(x) - \frac{v}{n} [\nabla\prox_{\phi_\theta}(x_{old} - v)]$;<br>
&emsp;  *update*($\theta$);<br>
**} return** $x$

******

Based on the observations 1.-7. above, it is easy to verify that for any vector $u\in\R^p$
$$
  \nabla\prox_{\phi_\theta}(u) = 
  \Vert \prox_{\phi_\theta}(u) \Vert_0^\ast,
  \quad\text{where}\quad
  \norm{u}_0^\ast:= 
  \#\{ \text{unique non-zero magintudes in } \abs{u} \}.
$$
E.g., $\norm{(0,3,-3,3,1)}_0^\ast = 2$.
Therefore, $v$ update in the AMP-SLOPE algorithm can be read as
$$
 v = \nabla g(x) - \frac{v}{n} \norm{x}_0^\ast.
$$

# Numerical experiments


## References
