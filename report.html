<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Bartłomiej Polaczyk" />

<meta name="date" content="2020-07-02" />

<title>AMP for SLOPE</title>

<script src="report_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="report_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="report_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="report_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="report_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="report_files/navigation-1.1/tabsets.js"></script>
<link href="report_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="report_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>


<style type="text/css">
  p.abstract{
    text-align: center;
    font-weight: bold;
  }
  div.abstract{
    margin: auto;
    width: 90%;
  }
</style>

<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="AMP-SLOPE.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">AMP for SLOPE</h1>
<h4 class="author"><em>Bartłomiej Polaczyk</em></h4>
<h4 class="date"><em>02 July 2020</em></h4>
<div class="abstract">
<p class="abstract">Abstract</p>
<p>The aim of this project is to investigate the the approximate message passing algorithm for SLOPE regularization problem based on <span class="citation">(Bu et al. 2019)</span> and compare it with classical convex optimization methods. Some numerical experiments regarding the cases that do not fit into the theoretical framework of <span class="citation">(Bu et al. 2019)</span> are also performed and analyzed.</p>
</div>

</div>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<div id="theoretical-bacground" class="section level2">
<h2>Theoretical bacground</h2>
<div id="introduction" class="section level3">
<h3>Introduction</h3>
We are interested in solving the standard linear inverse problem
<span class="math display">\[\begin{equation}\label{eq:LM-problem} 
  y = Ax + w,
\end{equation}\]</span>
<p>where <span class="math inline">\(y\in {\mathbb{R}}^n\)</span> is and <span class="math inline">\(A\in{\mathbb{R}}^{n\times p}\)</span> are known parameters of the model, <span class="math inline">\(w\in{\mathbb{R}}^n\)</span> is a random noise vector and <span class="math inline">\(x\in{\mathbb{R}}^p\)</span> is an unknown vector of paramteres we wish to estimate. We assume <span class="math inline">\(p\gg n\)</span>, i.e. the number of features is much greater than the size of the sample data and whence there might be many potential solutions to the problem<span class="math inline">\(~\eqref{eq:LM-problem}\)</span>.</p>
To resolve this issue and prevent overfitting, we introduce a penalty function <span class="math inline">\(\phi\)</span> which faforizes sparse solutions of<span class="math inline">\(~\eqref{eq:LM-problem}\)</span>, i.e. now we are looking among the minimizers of the following form
<span class="math display">\[\begin{equation}
  \widehat{x} = \operatorname*{argmin}_x \{\, \Vert Ax - y \Vert_2^2 + \phi(x) \,\}.
\end{equation}\]</span>
The usual choices of <span class="math inline">\(\phi\)</span> are scaled <span class="math inline">\(l^2\)</span> penalty (Tikhonov regularization) and <span class="math inline">\(l^1\)</span> penalty (LASSO). This note concerns SLOPE regularization, introduced for the first time in <span class="citation">(Bogdan et al. 2015)</span>, which assumes <span class="math inline">\(\phi\)</span> to be the sorted <span class="math inline">\(l^1\)</span> penalty, i.e.
<span class="math display">\[\begin{equation}
  \phi(x)= \phi_{\theta}(x) = \sum_{i=1}^n \theta_ix_i^{\downarrow},
\end{equation}\]</span>
<p>where <span class="math inline">\(x_1^\downarrow \ge x_2^\downarrow \ge \ldots \ge x_n^\downarrow\)</span> is the ordered permutation of the vector <span class="math inline">\((\vert x_1\vert,\vert x_2\vert,\ldots,\vert x_n\vert)\)</span> and <span class="math inline">\(\theta\)</span> is ahyperparameter of the model. Such a choice is a generalization of the <span class="math inline">\(l^1\)</span> regularization, as can be seen by taking <span class="math inline">\(\theta_i=\operatorname{const}\)</span>.</p>
<p>The fact that <span class="math inline">\(\phi_\theta\)</span> is non-separable makes the analysis of its teoretical properties much more onerous than in case of classical (separable) models, cf. <span class="citation">(Bogdan et al. 2015, <span class="citation">Bu et al. (2019)</span>)</span>. Nonetheless, it turns out that SLOPE has two advantages over other regularization methods such as LASSO and knocoffs, namely:</p>
<ol style="list-style-type: decimal">
<li>it achieves certain minimax estimation properties under particular random designs <em>without</em> requiring any knowledge of the sparsity degree of <span class="math inline">\(\widehat{x}\)</span>, cf. <span class="citation">(Bellec, Lecué, and Tsybakov 2018)</span></li>
<li>it controls the false discovery rate in the case of independent predictors, cf. <span class="citation">(Bogdan et al. 2015)</span>.</li>
</ol>
</div>
<div id="amp" class="section level3">
<h3>AMP</h3>
<p><span class="citation">Donoho, Maleki, and Montanari (2009)</span>, <span class="citation">Bu et al. (2019)</span>, <span class="citation">Zdeborová and Krzakala (2016)</span>, <span class="citation">Bayati and Montanari (2011)</span>.</p>
</div>
<div id="convex-optimization-methods" class="section level3">
<h3>Convex optimization methods</h3>
</div>
</div>
<div id="numerical-experiments" class="section level2">
<h2>Numerical experiments</h2>
<div id="comparison-with-convex-optimization-methods" class="section level3">
<h3>Comparison with convex optimization methods</h3>
</div>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-MR2810285">
<p>Bayati, M., and A. Montanari. 2011. “The Dynamics of Message Passing on Dense Graphs, with Applications to Compressed Sensing.” <em>IEEE Trans. Inform. Theory</em> 57 (2): 764–85. doi:<a href="https://doi.org/10.1109/TIT.2010.2094817">10.1109/TIT.2010.2094817</a>.</p>
</div>
<div id="ref-MR3852663">
<p>Bellec, P. C., G. Lecué, and A. B. Tsybakov. 2018. “Slope Meets Lasso: Improved Oracle Bounds and Optimality.” <em>Ann. Statist.</em> 46 (6B): 3603–42. doi:<a href="https://doi.org/10.1214/17-AOS1670">10.1214/17-AOS1670</a>.</p>
</div>
<div id="ref-MR3418717">
<p>Bogdan, M., E. van den Berg, C. Sabatti, W. Su, and E. J. Candès. 2015. “SLOPE—adaptive Variable Selection via Convex Optimization.” <em>Ann. Appl. Stat.</em> 9 (3): 1103–40. doi:<a href="https://doi.org/10.1214/15-AOAS842">10.1214/15-AOAS842</a>.</p>
</div>
<div id="ref-bu2019algorithmic">
<p>Bu, Z., J. Klusowski, C. Rush, and W. Su. 2019. “Algorithmic Analysis and Statistical Estimation of Slope via Approximate Message Passing.” In <em>Advances in Neural Information Processing Systems</em>, 9366–76. <a href="http://par.nsf.gov/biblio/10163278" class="uri">http://par.nsf.gov/biblio/10163278</a>.</p>
</div>
<div id="ref-donoho2009message">
<p>Donoho, D., A. Maleki, and A. Montanari. 2009. “Message-Passing Algorithms for Compressed Sensing.” <em>Proceedings of the National Academy of Sciences</em> 106 (45). National Acad Sciences: 18914–9. doi:<a href="https://doi.org/https://doi.org/10.1073/pnas.0909892106">https://doi.org/10.1073/pnas.0909892106</a>.</p>
</div>
<div id="ref-zdeborova2016statistical">
<p>Zdeborová, L., and F. Krzakala. 2016. “Statistical Physics of Inference: Thresholds and Algorithms.” <em>Advances in Physics</em> 65 (5). Taylor &amp; Francis: 453–552. doi:<a href="https://doi.org/https://doi.org/10.1080/00018732.2016.1211393">https://doi.org/10.1080/00018732.2016.1211393</a>.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "report_files/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
