<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Bartłomiej Polaczyk" />

<meta name="date" content="2020-07-10" />

<title>AMP for SLOPE</title>

<script src="report_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="report_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="report_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="report_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="report_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="report_files/navigation-1.1/tabsets.js"></script>
<link href="report_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="report_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>


<style type="text/css">
  p.abstract{
    text-align: center;
    font-weight: bold;
  }
  div.abstract{
    margin: auto;
    width: 90%;
  }
</style>

<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="AMP-SLOPE.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">AMP for SLOPE</h1>
<h4 class="author"><em>Bartłomiej Polaczyk</em></h4>
<h4 class="date"><em>10 July 2020</em></h4>
<div class="abstract">
<p class="abstract">Abstract</p>
<p>The aim of this project is to investigate the the approximate message passing algorithm for SLOPE regularization problem based on <span class="citation">(Bu et al. 2019)</span> and compare it with classical convex optimization methods. Some numerical experiments regarding the cases that do not fit into the theoretical framework of <span class="citation">(Bu et al. 2019)</span> are also performed and analyzed.</p>
</div>

</div>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<div id="theoretical-bacground" class="section level1">
<h1>Theoretical bacground</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
We are interested in solving the standard linear inverse problem
<span class="math display">\[\begin{equation}\label{eq:LM-problem} 
  y = Ax + w,
\end{equation}\]</span>
<p>where <span class="math inline">\(y\in {\mathbb{R}}^n\)</span> and <span class="math inline">\(A\in{\mathbb{R}}^{n\times p}\)</span> are known parameters of the model, <span class="math inline">\(w\in{\mathbb{R}}^n\)</span> is a random noise vector and <span class="math inline">\(x\in{\mathbb{R}}^p\)</span> is an unknown vector of paramteres we wish to estimate. We assume <span class="math inline">\(p\gg n\)</span>, i.e. the number of features is much greater than the size of the sample data and whence there might be many potential solutions to the problem<span class="math inline">\(~\eqref{eq:LM-problem}\)</span>.</p>
To resolve this issue and prevent overfitting, we introduce a penalty function <span class="math inline">\(\phi\)</span> which faforizes sparse solutions of<span class="math inline">\(~\eqref{eq:LM-problem}\)</span>, i.e. now we are looking among the minimizers of the following form
<span class="math display">\[\begin{equation}\label{eq:SLOPE}
  \widehat{x} = \operatorname*{argmin}_x \{\, \frac{1}{2}\Vert Ax - y \Vert_2^2 + \phi(x) \,\}.
\end{equation}\]</span>
<p>The usual choices of <span class="math inline">\(\phi\)</span> are scaled <span class="math inline">\(l^2\)</span> penalty (Tikhonov regularization) and <span class="math inline">\(l^1\)</span> penalty (LASSO). This note concerns sorted <span class="math inline">\(l^1\)</span> penalized estimation (abbrev. SLOPE), introduced for the first time in <span class="citation">(Bogdan et al. 2015)</span>, which assumes <span class="math inline">\(\phi\)</span> to be the sorted <span class="math inline">\(l^1\)</span> penalty, i.e. <span class="math display">\[
  \phi(x)= \phi_{\theta}(x) = \sum_{i=1}^n \theta_ix_i^{\downarrow},
\]</span> where <span class="math inline">\(x_1^\downarrow \ge x_2^\downarrow \ge \ldots \ge x_n^\downarrow\)</span> is the ordered permutation of the vector <span class="math inline">\({\left\vert x \right\vert}=({\left\vert x_1 \right\vert},{\left\vert x_2 \right\vert},\ldots,{\left\vert x_n \right\vert})\)</span> and <span class="math inline">\(\theta_1 \ge \theta_2 \ge \ldots \theta_p \ge 0\)</span> are hyperparameters of the model. To lighten notation, we denote <span class="math inline">\(\Delta_p = \{\, x\in{\mathbb{R}}^p\colon x_1\ge x_2 \ge \ldots \ge x_p \ge 0 \,\}\)</span>, so that the above requirements read: <span class="math inline">\(x^\downarrow,\theta\in\Delta_p\)</span>, where <span class="math inline">\(x^\downarrow = P{\left\vert x \right\vert}\)</span> for some permutation <span class="math inline">\(P\)</span> of the set <span class="math inline">\(\{1,2,\ldots,p\}\)</span>. Such a choice of regulizer is a generalization of the <span class="math inline">\(l^1\)</span> regularization, as can be seen by taking <span class="math inline">\(\theta_i\equiv\operatorname{const}\)</span>.</p>
<p>The fact that <span class="math inline">\(\phi_\theta\)</span> is non-separable makes the analysis of its teoretical properties much more onerous than in case of classical (separable) models, cf. <span class="citation">(Bogdan et al. 2015, <span class="citation">Bu et al. (2019)</span>)</span>. Nonetheless, it turns out that SLOPE has two advantages over other regularization methods such as LASSO and knocoffs, namely:</p>
<ol style="list-style-type: decimal">
<li>it achieves certain minimax estimation properties under particular random designs <em>without</em> requiring any knowledge of the sparsity degree of <span class="math inline">\(\widehat{x}\)</span>, cf. <span class="citation">(Bellec, Lecué, and Tsybakov 2018)</span>;</li>
<li>it controls the false discovery rate in the case of independent predictors, cf. <span class="citation">(Bogdan et al. 2015)</span>.</li>
</ol>
<p>We are interested in the algorithmic solutions to the problem <span class="math inline">\(\eqref{eq:SLOPE}\)</span>. Since the objective function in <span class="math inline">\(\eqref{eq:SLOPE}\)</span> is convex but not smooth, one can not apply directly the classical gradient descent and has to turn to other methods.</p>
<p>A natural alternative solution is the plethora of proximal algorithms, e.g. ISTA (and its improvement – FISTA, cf. <span class="citation">(A. Beck and Teboulle 2009)</span>) or more classical alternating direction methods of multipliers (ADMM). The methods have been throughly studied in the literature, cf. <span class="citation">(Beck A 2017)</span> for a detailed treatment of the subject.</p>
<p>In this note we will focus on another approach, via the approximate message passing, considered for the first time in context of the LASSO problem in <span class="citation">(Donoho, Maleki, and Montanari 2009)</span> and subsequentially developed in e.g. <span class="citation">(Bayati and Montanari 2011)</span>, and for the SLOPE regularization in <span class="citation">(Bu et al. 2019)</span> – see e.g. <span class="citation">(Zdeborová and Krzakala 2016)</span> for an accessible derivation of the method.</p>
<p>In the subsequent sections we will describre briefly some of these approaches.</p>
</div>
<div id="proximal-methods" class="section level2">
<h2>Proximal methods</h2>
<p>Denoting <span class="math inline">\(g(x)=\frac{1}{2}\Vert Ax - y \Vert_2^2\)</span>, the iterative shrinkage thresholding algorithm (ISTA) iteration for SLOPE can be written as:</p>
<hr />
<p><strong>ISTA:</strong><br> Initialize <span class="math inline">\(x\in{\mathbb{R}}^p\)</span>, <span class="math inline">\(\theta\in\Delta_p\)</span> <br> <strong>while</strong> (<em>stopping condition</em>) <strong>{</strong> <br>   <span class="math inline">\(x = {\operatorname{prox}}_{\phi_\theta}\big(x - \nabla g(x)\big)\)</span>;<br>   <em>update</em>(<span class="math inline">\(\theta\)</span>);<br> <strong>} return</strong> <span class="math inline">\(x\)</span></p>
<hr />
<p>where <span class="math inline">\(\theta\)</span> can be thought of as the learning rate, <em>update</em> is an appropriate backtracking procedure and <span class="math inline">\(\operatorname{prox}\)</span> denotes the proximal operator given by <span class="math display">\[
  {\operatorname{prox}}_{h}(y) := \operatorname*{argmin}_x \{\, h(x) + \frac{1}{2}\Vert x-y \Vert_2^2 \,\}.
\]</span></p>
<p><span class="citation">A. Beck and Teboulle (2009)</span> have introduced a faster version of ISTA, a.k.a. FISTA, which is based on the idea of Nesterov momentum. The general form of the algorithm is the following:</p>
<hr />
<p><strong>FISTA:</strong><br> Initialize <span class="math inline">\(x=x_{old}\in{\mathbb{R}}^p\)</span>, <span class="math inline">\(\theta \in \Delta_p\)</span>, <span class="math inline">\(t = 1\)</span><br> <strong>while</strong> <em>(stopping condition)</em> <strong>{</strong><br>   <span class="math inline">\(u = x_{old} + t(x-x_{old})\)</span>;<br>   <span class="math inline">\(x_{old}=x\)</span>;<br>   <span class="math inline">\(x = {\operatorname{prox}}_{\phi_\theta}\big(u - \nabla g(u)\big)\)</span>;<br>   <em>update</em>(<span class="math inline">\(\theta, t\)</span>);<br> <strong>} return</strong> <span class="math inline">\(x\)</span></p>
<hr />
<p>Here <span class="math inline">\(t\)</span> can be thought of as a acceleration term, which (if updated correctly) can increase substantialy the speed of convergence of the algorithm. Note that keeping <span class="math inline">\(t\equiv 1\)</span> restores ISTA.</p>
<p>One of the difficulties in dealing with SLOPE is that the regulizer <span class="math inline">\(\phi_\theta\)</span> is non-separable and thus its proximal operator cannot be applied element-wise. <span class="citation">(Bogdan et al. 2015)</span> have proposed an efficient algorithm that computes <span class="math display">\[
 \widehat{x}=
 {\operatorname{prox}}_{\phi_{\theta}}(y) = 
 \operatorname*{argmin}_x \big\{ \frac{1}{2}{\left\Vert y-x \right\Vert}^2 + \sum_i \theta_ix_i^\downarrow \big\}
 =
 \operatorname*{argmin}_x \big\{\sum_i\big[ \frac{1}{2}(x^\downarrow_i)^2 + x_i^\downarrow\theta_i - x_iy_i\big]\big\}
\]</span> It is based on the following simple observations that follow immediatly from the above formulation:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\({\operatorname{sign}}(\widehat{x}_i)={\operatorname{sign}}(y_i)\)</span> for each <span class="math inline">\(i\)</span> such that <span class="math inline">\(\widehat{x}_i\neq 0\)</span>;</li>
<li><span class="math inline">\(P\widehat{x}={\operatorname{prox}}_{\phi_\theta}(Py)\)</span> for any permutation <span class="math inline">\(P\)</span>;</li>
<li>If <span class="math inline">\(y\in \Delta_p\)</span>, then <span class="math inline">\(\widehat{x}\in\Delta_p\)</span> (i.e., <span class="math inline">\(y=y^\downarrow \Rightarrow \widehat{x}=\widehat{x}^\downarrow\)</span>);</li>
</ol>
<p>Therefore we can and do assume in the analysis below that <span class="math inline">\(y\in\Delta_p\)</span>. The optimization procedure now reads: <span class="math display">\[
  \widehat{x} = 
  \operatorname*{argmin}_{x^\downarrow\in\Delta_p} \big\{\sum_i\big[ \frac{1}{2}(x^\downarrow_i)^2 - x_i^\downarrow(y-\theta)_i \big]\big\},
\]</span> whence</p>
<ol start="4" style="list-style-type: decimal">
<li><span class="math inline">\(\widehat{x}\)</span> depends only on the vector <span class="math inline">\((\theta - y)\)</span>;</li>
<li>If <span class="math inline">\((y - \theta)_+\in\Delta_p\)</span> then <span class="math inline">\(\widehat{x} = (y - \theta)_+\)</span>, where <span class="math inline">\(v_+ = (\max(v_i,0))_i\)</span>;</li>
<li>If <span class="math inline">\((y-\theta)_i \le (y-\theta)_{i+1}\)</span>, then <span class="math inline">\(\widehat{x}_i \le \widehat{x}_{i+1}\)</span>, whence <span class="math inline">\(\widehat{x}_i = \widehat{x}_{i+1}\)</span>;</li>
<li>Consequently, if <span class="math inline">\((y-\theta)\)</span> is nondecreasing along the indices <span class="math inline">\((i,i+1,\ldots,j)\)</span>, then <span class="math inline">\(\widehat{x}_i = \widehat{x}_{i+1}= \ldots = \widehat{x}_{i+1}\)</span> and <span class="math display">\[
  \sum_{i\le r \le j}  \widehat{x}_r^\downarrow(y-\theta)_r 
  =
  \sum_{i\le r \le j} \widehat{x}_r^\downarrow(\bar{y}-\bar{\theta} ) \,
\]</span> where <span class="math inline">\(\bar{y},\bar{\theta}\)</span> are the means of <span class="math inline">\(y\)</span> and <span class="math inline">\(\theta\)</span> along the indices <span class="math inline">\((i,i+1,\ldots,j)\)</span>, so replacing <span class="math inline">\(y_r,\theta_r\)</span> with <span class="math inline">\(\bar{y},\bar{\theta}\)</span> does not change <span class="math inline">\(\widehat{x}\)</span>.</li>
</ol>
<p>The above observations justify the procedure below proposed by <span class="citation">(Bogdan et al. 2015)</span>.</p>
<hr />
<p><strong>FastProxSL1:</strong><br> <strong>Input:</strong> <span class="math inline">\(y\in{\mathbb{R}}^p\)</span>, <span class="math inline">\(\theta\in\Delta_p\)</span><br> # Define the operator <span class="math inline">\(H_y(v) = P({\operatorname{sign}}(y_i)v_i)_i\)</span> for some permutation <span class="math inline">\(P\)</span>, so that <span class="math inline">\(H_y(y) = y^\downarrow\in\Delta_p\)</span><br> <span class="math inline">\(y&#39; = H_y(y)\)</span>;<br> <strong>while</strong> <span class="math inline">\((y&#39;-\theta)_+\notin\Delta_p\)</span> <strong>{</strong><br>   identify nondecreasing and nonconstant segments <span class="math inline">\(i:j\)</span> of <span class="math inline">\((y&#39;-\theta)\)</span><br>   replace <span class="math inline">\(y&#39;_r,\theta_r\)</span> for <span class="math inline">\(r\in\{i,i+1,\ldots,j\}\)</span> by their averages <span class="math inline">\(\bar{y&#39;},\bar{\theta}\)</span> <br> <strong>} return</strong> <span class="math inline">\(H_y^{-1}(y&#39;-\theta)_+\)</span>;</p>
<hr />
</div>
<div id="approximate-message-passing" class="section level2">
<h2>Approximate message passing</h2>
</div>
</div>
<div id="numerical-experiments" class="section level1">
<h1>Numerical experiments</h1>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-MR2810285">
<p>Bayati, M., and A. Montanari. 2011. “The Dynamics of Message Passing on Dense Graphs, with Applications to Compressed Sensing.” <em>IEEE Trans. Inform. Theory</em> 57 (2): 764–85. doi:<a href="https://doi.org/10.1109/TIT.2010.2094817">10.1109/TIT.2010.2094817</a>.</p>
</div>
<div id="ref-MR2486527">
<p>Beck, A., and M. Teboulle. 2009. “A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems.” <em>SIAM J. Imaging Sci.</em> 2 (1): 183–202. doi:<a href="https://doi.org/10.1137/080716542">10.1137/080716542</a>.</p>
</div>
<div id="ref-MR3719240">
<p>Beck, A. 2017. <em>First-Order Methods in Optimization</em>. Vol. 25. MOS-Siam Series on Optimization. Society for Industrial; Applied Mathematics (SIAM), Philadelphia, PA; Mathematical Optimization Society, Philadelphia, PA. doi:<a href="https://doi.org/10.1137/1.9781611974997.ch1">10.1137/1.9781611974997.ch1</a>.</p>
</div>
<div id="ref-MR3852663">
<p>Bellec, P. C., G. Lecué, and A. B. Tsybakov. 2018. “Slope Meets Lasso: Improved Oracle Bounds and Optimality.” <em>Ann. Statist.</em> 46 (6B): 3603–42. doi:<a href="https://doi.org/10.1214/17-AOS1670">10.1214/17-AOS1670</a>.</p>
</div>
<div id="ref-MR3418717">
<p>Bogdan, M., E. van den Berg, C. Sabatti, W. Su, and E. J. Candès. 2015. “SLOPE—adaptive Variable Selection via Convex Optimization.” <em>Ann. Appl. Stat.</em> 9 (3): 1103–40. doi:<a href="https://doi.org/10.1214/15-AOAS842">10.1214/15-AOAS842</a>.</p>
</div>
<div id="ref-bu2019algorithmic">
<p>Bu, Z., J. Klusowski, C. Rush, and W. Su. 2019. “Algorithmic Analysis and Statistical Estimation of Slope via Approximate Message Passing.” In <em>Advances in Neural Information Processing Systems</em>, 9366–76. <a href="http://par.nsf.gov/biblio/10163278" class="uri">http://par.nsf.gov/biblio/10163278</a>.</p>
</div>
<div id="ref-donoho2009message">
<p>Donoho, D., A. Maleki, and A. Montanari. 2009. “Message-Passing Algorithms for Compressed Sensing.” <em>Proceedings of the National Academy of Sciences</em> 106 (45). National Acad Sciences: 18914–9. doi:<a href="https://doi.org/https://doi.org/10.1073/pnas.0909892106">https://doi.org/10.1073/pnas.0909892106</a>.</p>
</div>
<div id="ref-zdeborova2016statistical">
<p>Zdeborová, L., and F. Krzakala. 2016. “Statistical Physics of Inference: Thresholds and Algorithms.” <em>Advances in Physics</em> 65 (5). Taylor &amp; Francis: 453–552. doi:<a href="https://doi.org/https://doi.org/10.1080/00018732.2016.1211393">https://doi.org/10.1080/00018732.2016.1211393</a>.</p>
</div>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "report_files/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
